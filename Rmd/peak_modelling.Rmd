---
title: "peak_modelling"
output: 
  html_document:
    toc: true
    code_folding: hide
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      cache = TRUE,
                      message = FALSE)
```

## Libraries, Data import

```{r}
library(magrittr)
library(coda)
library(rstan)
library(stringr)
library(tidyverse)
```

```{r}
dat_files = list.files('~/ayush/data/', pattern = 'CombinedMatrix')
dat = map(dat_files,
          ~mutate(read_tsv(paste0('~/ayush/data/', .x), 
                           col_names = FALSE),
                  file = .x)) %>% 
  bind_rows %>% 
  select(file, X1, everything()) %>% 
  gather(subj, peak, X2:X25) %>% 
  mutate(subj = subj %>% gsub('X', '', .) %>% as.numeric %>% {. - 1},
         subj_type = c('case', 'ctrl')[(subj > 12) + 1],
         subj = factor(subj),
         peak = peak > 0,
         file = gsub('CombinedMatrix-', '', file)) %>% 
  rename(region = X1)
```

## Count correlation 

```{r}
peak_counts = dat %>% 
  group_by(file, region, subj_type) %>% 
  summarise(n_peaks = sum(peak)) %>% 
  ungroup
```

```{r}
peak_counts %>% 
  ggplot(aes(n_peaks)) + 
  geom_bar() +  
  facet_grid(subj_type ~ file)

peak_counts %>% 
  spread(subj_type, n_peaks) %>% 
  ggplot(aes(ctrl, case)) + 
  geom_jitter(alpha = .1) + 
  facet_wrap('file')
```

Incorporating this correlation into the model is important.


## Model

```{r, eval = FALSE}
data_frame(x = seq(0, 10, .01), y= dgamma(x, .1, .1)) %>% 
  ggplot(aes(x,y)) + geom_line()

stan_model_code = '
data {
  int<lower=0> N; // number of regions
  int case_counts[N];
  int ctrl_counts[N];
  
}
parameters {
  vector<lower=0, upper=1>[N] case_p;
  vector<lower=0, upper=1>[N] ctrl_p;
  real<lower=0> ctrl_a;
  real<lower=0> ctrl_b;
  real<lower=0> case_a;
  real<lower=0> case_b;
}
model {
  ctrl_a ~ gamma(.01, .01);
  ctrl_b ~ gamma(.01, .01);
  case_a ~ gamma(.01, .01);
  case_b ~ gamma(.01, .01);
  ctrl_p ~ beta(ctrl_a, ctrl_b);
  case_p ~ beta(case_a, case_b);
  case_counts ~ binomial(12, case_p);
  ctrl_counts ~ binomial(12, ctrl_p);
}
generated quantities {
  vector[N] case_ctrl_diff;
  case_ctrl_diff = case_p - ctrl_p;
}
'
# stanDSO = stan_model(model_code = stan_model_code)
```

The nice thing about this model is that it looks at the JOINT parameter space, rather than analyzing each region as if they were independent tests (they arent').

The downside is that because we're looking at the joint parameter space, there's `2 + 2 + 2*n_regions` parameters, so up to 120,084 for the E14 state. This makes it pretty slow to fit on my laptop. Some timing tests:

```{r}

test_timing = function(n_region){
  tmp_dat = peak_counts %>% 
    filter(file == 'E1-10000bps.txt') %>% 
    spread(subj_type, n_peaks) %>% 
    sample_n(n_region)
  
  mcmc_time = microbenchmark(tmp_sample <- sampling(stanDSO, 
                                                    data = list(N = nrow(tmp_dat), case_counts = tmp_dat$case, ctrl_counts = tmp_dat$ctrl),
                                                    chains = 3,
                                                    warmup = 500,
                                                    cores = 3),
                             times = 1)$time * 1e-9
  
  
  vb_time = microbenchmark(tmp_vb <- vb(object = stanDSO,
                                        data = list(N = nrow(tmp_dat), case_counts = tmp_dat$case, ctrl_counts = tmp_dat$ctrl)),
                           times = 1)$time * 1e-9
  
  timing_test_res = data_frame(method = c('mcmc', 'vb'),
                               n_region = n_region,
                               time_seconds = c(mcmc_time, vb_time))
  return(timing_test_res)
}

# timing_test_res = map(c(100, 200 ,500, 1000, 2000, 4000),
#     test_timing) %>% 
#   bind_rows

load('~/ayush/outputs/timing_test_res.RData')

timing_test_res %>% 
  ggplot(aes(n_region, time_seconds)) +
  geom_line(aes(color = method)) +
  geom_point(aes(color = method))
```

Fortunately, a variational approximation will fit in a reasonable time on my laptop. If we want to take the time to run the actual MCMC we can probably do so, it will just take a few hours/days (probably not weeks). 

```{r, eval = FALSE}
# res = tmp_sample %>% rstan::extract() %>% .[[7]] %>% as_tibble %>% set_names(tmp_dat$region) %>% gather(region, case_ctrl_diff)
# 
# res %<>% 
#   group_by(region) %>% 
#   summarise(mean_diff = mean(case_ctrl_diff), 
#             hdi95 = list(HPDinterval(mcmc(case_ctrl_diff)))) %>% 
#   arrange(desc(abs(mean_diff))) %>% 
#   mutate(lower = map_dbl(hdi95, ~.x[1]),
#          upper = map_dbl(hdi95, ~.x[2]),
#          is_different = map_lgl(hdi95, ~!between(0, .x[1], .x[2])))
# 
# tmp_sample %>% rstan::extract() %>% .[3:6] %>% map(as_tibble) %>% bind_cols %>% set_names(c('ctrl_a', 'ctrl_b', 'case_a', 'case_b')) %>% colMeans
# 
# data_frame(x = seq(0, 1, .01), ctrl = dbeta(x, .571, 5.49), case = dbeta(x, 1.463, 11.64)) %>% gather(subj_type, val, -x) %>% ggplot(aes(x, val)) + geom_line(aes(color = subj_type))
# 
# tmp_sample %>% rstan::extract() %>% .[[7]] %>% as_tibble %>% select(916) %>% ggplot(aes(V916)) + geom_histogram(bins = 40)

# e12_full = peak_counts %>% 
#   filter(file == 'E12-10000bps.txt') %>% 
#   spread(subj_type, n_peaks)
# 
# microbenchmark(e12_full_vb <- vb(object = stanDSO,
#                                         data = list(N = nrow(e12_full), case_counts = e12_full$case, ctrl_counts = e12_full$ctrl)),
#                            times = 1) #147 seconds

# e12_res = e12_full_vb %>% rstan::extract() %>% .[[7]] %>% as_tibble() %>% set_names(e12_full$region) %>% gather(region, case_ctrl_diff) %>% group_by(region) %>% summarise(region_diff_hdi = list(hdi_fun(case_ctrl_diff)))
# 
# e12_res %>% 
#   mutate(hdi_99_func = map_lgl(region_diff_hdi, ~!between(0, .x[1], .x[2]))) %>% 
#   filter(hdi_99_func) %>% 
#   left_join(e12_full, by = 'region')

hdi_fun = function(case_ctrl_diff_vec, prob = .99){
  HPDinterval(mcmc(case_ctrl_diff_vec), prob = prob)
}

run_vb = function(file_dat){
  strt_time = Sys.time()
  data_list = list(N = nrow(file_dat), case_counts = file_dat$case, ctrl_counts = file_dat$ctrl)

  vb_res = vb(object = stanDSO,
     data = data_list,
     output_samples = 2000)
  save(vb_res,
       file = paste0('~/ayush/outputs/', str_extract(file_dat$file[1], 'E[0-9]+'), '_hits.RData'))
  
  vb_summary = vb_res %>% 
    rstan::extract() %>% 
    .[[7]] %>% # case_ctrl_diff element
    as_tibble() %>%
    set_names(file_dat$region) %>% 
    gather(region, case_ctrl_diff) %>%
    group_by(region) %>% 
    summarise(region_diff_hdi = list(hdi_fun(case_ctrl_diff)))
  
  vb_hits = vb_summary %>% 
    mutate(hdi_99_functional = map_lgl(region_diff_hdi, ~!between(0, .x[1], .x[2]))) %>% 
    filter(hdi_99_functional) %>% 
    left_join(file_dat, by = 'region')
    
  return(vb_hits)
}

peak_counts %>% 
  spread(subj_type, n_peaks) %>% 
  group_by(file) %>% 
  nest %>% 
  mutate(data = map2(file, data, ~mutate(.y, file = .x))) %>% # I really wish dplyr had a better way of doing this
  mutate(vb_time = map(data, run_vb)) # runs variational bayes, saves the results, filters down to the hits, and returns those
  



```

## Show the hits by file

<This is where I will do that>
