---
title: "peak_modelling"
output: 
  html_document:
    toc: true
    code_folding: hide
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      cache = TRUE,
                      message = FALSE)
```

## Libraries, Data import

```{r}
library(knitr)
library(magrittr)
library(coda)
library(rstan)
library(stringr)
library(tidyverse)
```

```{r}
dat_files = list.files('~/ayush/data/', pattern = 'CombinedMatrix')
dat = map(dat_files,
          ~mutate(read_tsv(paste0('~/ayush/data/', .x), 
                           col_names = FALSE),
                  file = .x)) %>% 
  bind_rows %>% 
  select(file, X1, everything()) %>% 
  gather(subj, peak, X2:X25) %>% 
  mutate(subj = subj %>% gsub('X', '', .) %>% as.numeric %>% {. - 1},
         subj_type = c('case', 'ctrl')[(subj > 12) + 1],
         subj = factor(subj),
         peak = peak > 0,
         file = gsub('CombinedMatrix-', '', file)) %>% 
  rename(region = X1)
```

## Count correlation 

```{r}
peak_counts = dat %>% 
  group_by(file, region, subj_type) %>% 
  summarise(n_peaks = sum(peak)) %>% 
  ungroup

#save(peak_counts, file = '~/ayush/outputs/peak_counts.RData')
```

```{r}
peak_counts %>% 
  ggplot(aes(n_peaks)) + 
  geom_bar() +  
  facet_grid(subj_type ~ file)

peak_counts %>% 
  spread(subj_type, n_peaks) %>% 
  ggplot(aes(ctrl, case)) + 
  geom_jitter(alpha = .1) + 
  facet_wrap('file')
```

Incorporating this correlation into the model is important.

## Traditional tests

We can use Fisher's exact test to see if, for each region, the number of peaks is independent of case-ctrl status:

```{r, eval = FALSE}
fisher_test_region = function(region_dat){
  mat = t(as.matrix(cbind(region_dat$n_peaks, region_dat$n_nonpeaks)))
  
  fisher.test(mat)
}

region_fisher_tests = peak_counts %>% 
  mutate(n_nonpeaks = 12 - n_peaks) %>%
  group_by(file, region) %>% 
  nest %>%
  mutate(fisher_test = mclapply(data, fisher_test_region, mc.cores = 20),
         fisher_p = map_dbl(fisher_test, ~.x$p.value),
         fisher_q = p.adjust(fisher_p, method = 'fdr'))

```
```{r}
load('~/ayush/outputs/region_fisher_tests.RData')
region_fisher_tests %>% 
  filter(fisher_p < .05) %>% 
  arrange(fisher_p) %>% 
  head() %>% 
  select(-fisher_test) %>% 
  unnest %>% 
  kable
```


The corrections for multiple testing completely kills any measures of statistical significance, i.e. there are no regions with FDR Q < .05. There are 1334 regions with p < .05, but that's not very surprising when you test 183k regions.

The most statistically significant region `chr2:54680000-54689999` looks pretty promising in [the genome browser](https://genome.ucsc.edu/cgi-bin/hgTracks?db=hg19&lastVirtModeType=default&lastVirtModeExtraState=&virtModeType=default&virtMode=0&nonVirtPosition=&position=chr2%3A54680000%2D54689999&hgsid=658320433_bE4kAef1PcaGZ4mn4s0qo1qzyiLx) as it covers the start of SPTBN1 and shows a lot of histone marks. [This website](http://www.proteinatlas.org/ENSG00000115306-SPTBN1/pathology) seems to suggest that this gene is prognostic marker in renal cancer.

So there probably IS signal in this data, but this method of analysis makes heavy assumptions about the independence of the regions. These independence assumptions eliminate our ability to make any positive conclusions about signal in this data.

## Model

Here I define a Bayesian hierarchical model. 

```{r, eval = FALSE}
data_frame(x = seq(0, 10, .01), y= dgamma(x, .1, .1)) %>% 
  ggplot(aes(x,y)) + geom_line()

stan_model_code = '
data {
  int<lower=0> N; // number of regions
  int case_counts[N];
  int ctrl_counts[N];
  
}
parameters {
  vector<lower=0, upper=1>[N] case_p;
  vector<lower=0, upper=1>[N] ctrl_p;
  real<lower=0> ctrl_a;
  real<lower=0> ctrl_b;
  real<lower=0> case_a;
  real<lower=0> case_b;
}
model {
  ctrl_a ~ gamma(.01, .01);
  ctrl_b ~ gamma(.01, .01);
  case_a ~ gamma(.01, .01);
  case_b ~ gamma(.01, .01);
  ctrl_p ~ beta(ctrl_a, ctrl_b);
  case_p ~ beta(case_a, case_b);
  case_counts ~ binomial(12, case_p);
  ctrl_counts ~ binomial(12, ctrl_p);
}
generated quantities {
  vector[N] case_ctrl_diff;
  case_ctrl_diff = case_p - ctrl_p;
}
'
stanDSO = stan_model(model_code = stan_model_code)
```

The nice thing about this model is that it looks at the JOINT parameter space, rather than analyzing each region as if they were independent tests (they arent').

The downside is that because we're looking at the joint parameter space, there's `2 + 2 + 2*n_regions` parameters, so up to 120,084 for the E14 state. This makes it pretty slow to fit on my laptop. Some timing tests:

```{r}

test_timing = function(n_region){
  tmp_dat = peak_counts %>% 
    filter(file == 'E1-10000bps.txt') %>% 
    spread(subj_type, n_peaks) %>% 
    sample_n(n_region)
  
  mcmc_time = microbenchmark(tmp_sample <- sampling(stanDSO, 
                                                    data = list(N = nrow(tmp_dat), case_counts = tmp_dat$case, ctrl_counts = tmp_dat$ctrl),
                                                    chains = 3,
                                                    warmup = 500,
                                                    cores = 3),
                             times = 1)$time * 1e-9
  
  
  vb_time = microbenchmark(tmp_vb <- vb(object = stanDSO,
                                        data = list(N = nrow(tmp_dat), case_counts = tmp_dat$case, ctrl_counts = tmp_dat$ctrl)),
                           times = 1)$time * 1e-9
  
  timing_test_res = data_frame(method = c('mcmc', 'vb'),
                               n_region = n_region,
                               time_seconds = c(mcmc_time, vb_time))
  return(timing_test_res)
}

# timing_test_res = map(c(100, 200 ,500, 1000, 2000, 4000),
#     test_timing) %>% 
#   bind_rows

load('~/ayush/outputs/timing_test_res.RData')

timing_test_res %>% 
  ggplot(aes(n_region, time_seconds)) +
  geom_line(aes(color = method)) +
  geom_point(aes(color = method))
```

Fortunately, a variational approximation will fit in a reasonable time on my laptop. If we want to take the time to run the actual MCMC we can probably do so, it will just take a few hours/days (probably not weeks). 

```{r, eval = FALSE}
# res = tmp_sample %>% rstan::extract() %>% .[[7]] %>% as_tibble %>% set_names(tmp_dat$region) %>% gather(region, case_ctrl_diff)
# 
# res %<>% 
#   group_by(region) %>% 
#   summarise(mean_diff = mean(case_ctrl_diff), 
#             hdi95 = list(HPDinterval(mcmc(case_ctrl_diff)))) %>% 
#   arrange(desc(abs(mean_diff))) %>% 
#   mutate(lower = map_dbl(hdi95, ~.x[1]),
#          upper = map_dbl(hdi95, ~.x[2]),
#          is_different = map_lgl(hdi95, ~!between(0, .x[1], .x[2])))
# 
# tmp_sample %>% rstan::extract() %>% .[3:6] %>% map(as_tibble) %>% bind_cols %>% set_names(c('ctrl_a', 'ctrl_b', 'case_a', 'case_b')) %>% colMeans
# 
# data_frame(x = seq(0, 1, .01), ctrl = dbeta(x, .571, 5.49), case = dbeta(x, 1.463, 11.64)) %>% gather(subj_type, val, -x) %>% ggplot(aes(x, val)) + geom_line(aes(color = subj_type))
# 
# tmp_sample %>% rstan::extract() %>% .[[7]] %>% as_tibble %>% select(916) %>% ggplot(aes(V916)) + geom_histogram(bins = 40)

# e12_full = peak_counts %>% 
#   filter(file == 'E12-10000bps.txt') %>% 
#   spread(subj_type, n_peaks)
# 
# microbenchmark(e12_full_vb <- vb(object = stanDSO,
#                                         data = list(N = nrow(e12_full), case_counts = e12_full$case, ctrl_counts = e12_full$ctrl)),
#                            times = 1) #147 seconds

# e12_res = e12_full_vb %>% rstan::extract() %>% .[[7]] %>% as_tibble() %>% set_names(e12_full$region) %>% gather(region, case_ctrl_diff) %>% group_by(region) %>% summarise(region_diff_hdi = list(hdi_fun(case_ctrl_diff)))
# 
# e12_res %>% 
#   mutate(hdi_99_func = map_lgl(region_diff_hdi, ~!between(0, .x[1], .x[2]))) %>% 
#   filter(hdi_99_func) %>% 
#   left_join(e12_full, by = 'region')

hdi_fun = function(case_ctrl_diff_vec, prob = .99){
  HPDinterval(mcmc(case_ctrl_diff_vec), prob = prob)
}

run_vb = function(file_dat){
  strt_time = Sys.time()
  data_list = list(N = nrow(file_dat), case_counts = file_dat$case, ctrl_counts = file_dat$ctrl)

  vb_res = vb(object = stanDSO,
              data = data_list,
              output_samples = 2000)
  save(vb_res,
       file = paste0('~/ayush/outputs/rerun_outputs/', str_extract(file_dat$file[1], 'E[0-9]+'), '_hits.RData'))
  
  vb_summary = vb_res %>% 
    rstan::extract() %>% 
    .[[7]] %>% # case_ctrl_diff element
    as_tibble() %>%
    set_names(file_dat$region) %>% 
    gather(region, case_ctrl_diff) %>%
    group_by(region) %>% 
    summarise(region_diff_hdi = list(hdi_fun(case_ctrl_diff)))
  
  vb_hits = vb_summary %>% 
    mutate(hdi_99_functional = map_lgl(region_diff_hdi, ~!between(0, .x[1], .x[2]))) %>% 
    filter(hdi_99_functional) %>% 
    left_join(file_dat, by = 'region')
    
  return(vb_hits)
}

hits_by_file = peak_counts %>% 
  spread(subj_type, n_peaks) %>% 
  group_by(file) %>% 
  nest %>% 
  mutate(data = map2(file, data, ~mutate(.y, file = .x))) %>% # I really wish dplyr had a better way of doing this
  mutate(vb_hits = mclapply(data, run_vb, mc.cores = 4)) # runs variational bayes, saves the results, filters down to the hits, and returns those
  
save(hits_by_file,
     file = '~/ayush/outputs/hits_by_file.RData')


```

## Show the hits by file


```{r}
load('~/ayush/outputs/hits_by_file.RData')

format_hdi = function(hdi) {
  rounded = round(hdi, digits = 3)
  
  paste0(rounded[1], ' to ', rounded[2])
}

hits_by_file %>% 
  select(-data) %>% 
  unnest %>% 
  group_by(file) %>% summarise(n_hits = n()) %>% kable

hits_by_file %>% 
  select(-data) %>% 
  unnest %>% 
  group_by(file) %>% 
  do(head(.)) %>% 
  ungroup %>% 
  mutate(region_diff_hdi = map_chr(region_diff_hdi, format_hdi),
         file = gsub('-10000bps.txt', '', file)) %>% 
  select(-file1) %>% 
  kable(align = 'llcrrr')
```

<This is where I will do that>
